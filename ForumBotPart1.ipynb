{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": []
  }
     ]
    }
   ],
   "source": [
    "#Written by MrZiemni0k // PART 1\n",
    "#My 1st WebScraping Project. Still needs some improvement but at least it works.\n",
    "#Should add re.compile as argument for func geturlImgPage - maybe next time :)\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "import time\n",
    "\n",
    "\n",
    "#The Archive Page is 3-page deep(Main page, Sub page(Category) and threads with content. Typical forum.\n",
    "#The main goal for the 1st part of the script is to store all img urls hidden within bbcode(plain text).\n",
    "#Subpage urls stored in mainlist, threads in sublist and img urls in imglist.\n",
    "\n",
    "# Function to store thread urls in sublist.\n",
    "# Passed argument will be url for subpage stored in mainlist.\n",
    "def geturlThreadPage(pageUrl):\n",
    "    \n",
    "    n = 1                   #Page number to jump futher into subpage.\n",
    "    while True:             #Loop for increasing page number within subpage url with the same pattern .../{mainlist}-page-{number}.html\n",
    "        try:                #Trying to avoid error in the middle of script if there is any problem with connection.\n",
    "            \n",
    "            website = 'http://(somekindofwebpage).com/{}-page-{}.html'.format(pageUrl[:-5],n)  #Slicing {mainlist} to put \"-page-{number}\"\n",
    "            newweb = requests.get(website)      #Getting info about the website. \n",
    "            print(website)                      #Keeping track of the website url/num.\n",
    "            time.sleep(2.5)                     #Giving a rest for a server - Good, gut bot :3\n",
    "        except HTTPError: \n",
    "            return(\"HTTPError\")\n",
    "        except URLError:\n",
    "            return('The server could not be found!')\n",
    "             \n",
    "        bs = bs4.BeautifulSoup(newweb.text,\"lxml\")        #Converting {newweb} into readable form of html.\n",
    "        looplist = []                           #Empty list for breaking the loop if url is not found on empty page. \n",
    "                                                #Let's say subpage count is 50, so on 51 I want the loop to stop.\n",
    "        \n",
    "        for url in bs.find_all(\"li\"):                     #Urls are stored in tables - looking there.\n",
    "            if url.a != None:                             #If urls stored at \"a href=\" withing tables:\n",
    "                sublist.append(url.a[\"href\"])             #Add urls to sublist for the next func to work with later.\n",
    "                looplist.append(url.a[\"href\"])            #Add urls to looplist to chck later if any urls found within bs.find_all.\n",
    "        if len(looplist) == 0:                            #If looplist is empty/no stored urls then most probably we chcecked all pgs within subpage.\n",
    "            print(\"Breaking loop at{}\".format(website))   #Giving info to check progress.\n",
    "            time.sleep(2)                                 #Being gud, gut bot :3\n",
    "            break\n",
    "        else:\n",
    "            n += 1                                        #Urls found make n(page)+1\n",
    "\n",
    "# Function to store image_urls in imglist.\n",
    "# Passed arguments are:\n",
    "#                     url for thread stored in sublist\n",
    "#                     a list to store our imgurls. \n",
    "def geturlImgPage(urlsub,imglist):\n",
    "    \n",
    "    n = 1            #Page number to jump futher into subpage.\n",
    "    while True:      #Loop for increasing page number within thread url with the same pattern .../{sublist}-page-{number}.html\n",
    "        try:         #Trying to avoid error in the middle of script if there is any problem with connection.\n",
    "            \n",
    "            website = 'http://(somekindofwebpage).com{}-page-{}.html'.format(urlsub[:-5],n)  #Slicing {sublist} to put \"-page-{number}\"\n",
    "            newweb = requests.get(website)          #Getting info about the website.\n",
    "            print(website)                          #Keeping track of the website url/num.\n",
    "            time.sleep(2)                           #Giving a rest for a server - Good, gut bot :3\n",
    "        except HTTPError:\n",
    "            print(\"HTTPError\")\n",
    "        except URLError:\n",
    "            print('The server could not be found!')\n",
    "        \n",
    "        bs = bs4.BeautifulSoup(newweb.text,\"lxml\")                       #Converting {newweb} into readable form of html.\n",
    "        anytext = bs.find_all(\"div\", {\"class\":\"subtable altbg2 t_msg\"})  #The imgurls are hidden within broken bbcode in <div> \"subtable altbg2 t_msg\" - class. \n",
    "        if len(anytext) != 0:                              # If there are msges (not empty website):\n",
    "            for img in anytext:                                  # For text (img) extracted from bs.find_all\n",
    "                print(img.text)                                  # For tracking print list of imgurls in each message.\n",
    "                imglist += re.findall(specify, img.text)         # Add to the imglist only specified(specify-below/outside function) text based on img.text.\n",
    "# Could add slicing to get urls only/without [img] tags          # In our case [img](anything)[/img].lower(). Some posts are with [IMG]/[Img] etc.\n",
    "        else:                                              # If no msges (empty website)\n",
    "            print(\"Breaking loop at{}\".format(website))          # Print website/num at which we gonna break\n",
    "            break\n",
    "        n += 1                                          # Add +1 to thread page.\n",
    "\n",
    "    \n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                                Some lists/arguments \n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "mainlist = [] #Store subpages urls\n",
    "sublist = []  #Store threads urls\n",
    "imglist = []  #Store img urls\n",
    "specify = re.compile(\"\\[img\\]+\\S+\\[/img\\]\", re.IGNORECASE)  #Filter to find [img](anything)[/img].lower() for storing\n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                    Main page - looking for subpages (No func)\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#Checking main webpage for subpages. And adding them into mainlist.\n",
    "try:\n",
    "    strona = requests.get('(somekindofwebpage).com/archive')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print('The server could not be found!')\n",
    "\n",
    "\n",
    "soup = bs4.BeautifulSoup(strona.text,\"lxml\")\n",
    "for url in soup.find_all(\"li\"):\n",
    "    if url.a != None:\n",
    "        mainlist.append(url.a[\"href\"])\n",
    "\n",
    "print(mainlist)  # Keeping for futher tracking how many more left. Still possible to track.\n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                   Executing funcs and storing imgurl list in new file\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "for x in mainlist:\n",
    "    geturlThreadPage(x)\n",
    "\n",
    "print(sublist)\n",
    "print(\"Getting Thread URLs finished!\")\n",
    "time.sleep(10)\n",
    "\n",
    "for x in sublist:\n",
    "    geturlImgPage(x,imglist)\n",
    "    \n",
    "print(imglist)\n",
    "\n",
    "with open(\"imglist.txt\", \"w\") as output:\n",
    "    output.write(str(imglist))"
   ]
  },    
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
