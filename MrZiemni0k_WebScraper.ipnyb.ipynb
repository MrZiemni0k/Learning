{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Marcin \"MrZiemni0k\".\n",
    "\n",
    "My 1st WebScraping Project. Still needs some improvement but at least it works.\n",
    "Planning to add:\n",
    "* Interactions between an user and the script:\n",
    "    - Check for dead hosts by .netloc?\n",
    "    - Delete .netlock urls? \n",
    "    - Store timed out urls? \n",
    "    - Directory input?\n",
    "    - Minimum size of the pictures to be downloaded?\n",
    "* Compress code into nicer version. Add classes.\n",
    "* Make nested dictionary: {Sub page:{Thread:[Urls]}} for smoother interaction with an user. For example to let's say to avoid sub pages that are not interesting for us. \n",
    "* Adjust url patterns for web-crawlers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script is towards common public forum which is 3-page deep(Main Page, Sub Page(Categories) and Threads with a content \n",
    "we are looking for. My script's main goal is to download all images(with some adjustments) from urls hidden within bbcode.\n",
    "                                                             \n",
    "My target was a medium size japanese archive forum (12years of content). It took around 2 days to webscrape urls \n",
    "(Could be done faster depending on set value of time.sleep())\n",
    "and 4 days to download all images after some sorting and cleaning - Many hosts were slow or dead etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "import time\n",
    "\n",
    "def geturlThreadPage(pageUrl):\n",
    "#   Function to store thread urls in sublist.\n",
    "#   Passed argument will be url for subpage stored in mainlist.\n",
    "#   Pattern of urls in subpages is: .../{mainlist}-page-{number}.html\n",
    "    \n",
    "    num_page = 1                  \n",
    "    while True: \n",
    "        \n",
    "        try:                    \n",
    "            website = 'http://www.somekindofwebsite.jp//{}-page-{}.html'.format(pageUrl[:-5],num_page)  \n",
    "                                                                    #Slicing {mainlist} to put \"-page-{number}\"\n",
    "            readweb = requests.get(website)       \n",
    "            #time.sleep()                           #OPTIONAL! - Giving a rest for a server - Good, gut bot :3\n",
    "        except HTTPError: \n",
    "            return(\"HTTPError\")\n",
    "        except URLError:\n",
    "            return('The server could not be found!')\n",
    "        except:\n",
    "            return(\"No Connection\")\n",
    "             \n",
    "             \n",
    "        bs = bs4.BeautifulSoup(readweb.text,\"lxml\")        \n",
    "        looplist = []                           #Empty list for breaking the loop if url is not found on empty page. \n",
    "                                                #Let's say subpage count is 50, so on 51st page I want the loop to stop.\n",
    "        \n",
    "        for url in bs.find_all(\"li\"):                     #Urls are stored in html lists - looking there.\n",
    "            if url.a != None:                             #If urls stored at \"a href=\" withing tables:\n",
    "                sublist.append(url.a[\"href\"])             #Add urls to sublist for the next func to work with later.\n",
    "                looplist.append(url.a[\"href\"])            #Add urls to looplist to chck later if any urls found within bs.find_all.\n",
    "        if len(looplist) == 0:                            #If looplist is empty/no stored urls then most probably we chcecked all pgs within subpage.\n",
    "            print(\"Breaking loop at {}\".format(website))   #Giving info to check progress.\n",
    "            time.sleep(0.2)                                 #OPTIONAL! Being gud, gut bot :3\n",
    "            break\n",
    "        else:\n",
    "            num_page += 1                                        #Urls found make num_page+1\n",
    "\n",
    "\n",
    "            \n",
    "def geturlImgPage(urlsub,imglist):\n",
    "#Function to store image_urls in imglist list and file.\n",
    "#Passed arguments are:\n",
    "#urlsub for thread stored in sublist\n",
    "#a list to store our imgurls\n",
    "#Pattern:  .../{sublist}-page-{number}.html\n",
    "\n",
    "\n",
    "    def should_I_look_img():\n",
    "        '''\n",
    "        Assist Function to check 1st page of a thread for urls using regular expressions defined in specify.\n",
    "        '''\n",
    "        for img in anytext:                                  \n",
    "            findurl=re.search(specify,img.text)\n",
    "            if findurl:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def should_I_look_txt(): \n",
    "        '''\n",
    "        Assist Function to check if we are on the last page of a thread.\n",
    "        '''\n",
    "        for img in anytext:                                  \n",
    "            findurl=re.search(\"\\S+\",img.text)\n",
    "            if findurl:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    specify = re.compile(\"\\[img\\]+\\S+\\[/img\\]\", re.IGNORECASE)  #Filter to find [img](anything)[/img].lower()\n",
    "    chck_loop_url = 0                                           #Loop to skip no good content threads.\n",
    "    num_page = 1            \n",
    "    while True:       \n",
    "        try:       \n",
    "            \n",
    "            website = 'http://www.somekindofwebsite.jp//{}-page-{}.html'.format(urlsub[:-5],num_page)  \n",
    "                                                                #Slicing {sublist} to put \"-page-{number}\"\n",
    "            readweb = requests.get(website)          \n",
    "            #time.sleep()                           #OPTIONAL! Giving a rest for a server - Good, gut bot :3\n",
    "        except HTTPError:\n",
    "            return(\"HTTPError\")\n",
    "        except URLError:\n",
    "            return('The server could not be found!')\n",
    "        except:\n",
    "            return(\"No Connection\")\n",
    "        \n",
    "        bs = bs4.BeautifulSoup(readweb.text,\"lxml\")                       \n",
    "        anytext = bs.find_all(\"div\", {\"class\":\"subtable altbg2 t_msg\"})  \n",
    "        #Mesages (in our case - The imgurls) are hidden within broken bbcode in <div> \"subtable altbg2 t_msg\" - class.\n",
    "        \n",
    "        store_result = should_I_look_img()\n",
    "        \n",
    "        if num_page == 1 and store_result: #Chck for 1st page if any images found..                                   \n",
    "                                                         #If not then most probably no images at all in a thread. \n",
    "                                                         #Skip to not waste the time.              \n",
    "            for img in anytext:  \n",
    "                findurl=re.findall(specify,img.text)\n",
    "                for url in findurl:\n",
    "                    print(f'Adding: {url[5:-6]}')\n",
    "                    imglist.append(url[5:-6])\n",
    "                    f.write(url[5:-6] + '\\n')\n",
    "                    \n",
    "                    \n",
    "        elif num_page!=1 and should_I_look_txt():        \n",
    "                                                         #Chck if 3 pages in a row are without pictures.\n",
    "                                                         #Skip to not waste time. Some threads were over 1000pages.\n",
    "                                                         #Less probability to find anything useful on later pages.\n",
    "                    \n",
    "            if chck_loop_url == 2:\n",
    "                print(\"-!-!-!-!-!-!-!\")\n",
    "                print(\"Too many pages without image urls. Going next thread.\")\n",
    "                chck_loop_url = 0\n",
    "                del sublist[0]\n",
    "                break\n",
    "            \n",
    "            elif store_result:\n",
    "            \n",
    "                \n",
    "                for img in anytext:                                  \n",
    "                    findurl=re.findall(specify,img.text)\n",
    "                    for url in findurl:\n",
    "                        print(f'Adding: {url[5:-6]}')\n",
    "                        imglist.append(url[5:-6])\n",
    "                        chck_loop_url = 0\n",
    "                        f.write(url[5:-6] + '\\n')\n",
    "                        \n",
    "            \n",
    "            else:\n",
    "                chck_loop_url += 1\n",
    "               \n",
    "                                                                \n",
    "        elif num_page == 1:                                             \n",
    "            print(\"No Imgs Found on 1st Page. Going next thread.\")\n",
    "            del sublist[0]\n",
    "            break\n",
    "        else:                                              \n",
    "            print(\"No Page Found. Breaking loop at: {}\".format(website))         \n",
    "            del sublist[0]\n",
    "            break\n",
    "            \n",
    "        num_page += 1                                        \n",
    "        \n",
    "        \n",
    "    \n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                                Some lists/arguments \n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "mainlist = [] #Store subpages urls\n",
    "sublist = []  #Store threads urls\n",
    "imglist = []  #Store img urls\n",
    "f = open(\"Zdjecia.txt\", \"w\", encoding=\"utf-8\")\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                    Main page - looking for subpages (No func)\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#Checking main webpage for subpages. And adding them into mainlist.\n",
    "\n",
    "try:\n",
    "    website = requests.get('http://www.somekindofwebsite.jp/archiver/')\n",
    "except HTTPError:\n",
    "    print(\"HTTPError\")\n",
    "except URLError:\n",
    "    print('The server could not be found!')\n",
    "except:\n",
    "    print(\"No Connection\")\n",
    "        \n",
    "soup = bs4.BeautifulSoup(website.text,\"lxml\")\n",
    "for url in soup.find_all(\"li\"):\n",
    "    if url.a != None:\n",
    "        mainlist.append(url.a[\"href\"])\n",
    "\n",
    "print(mainlist)  \n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                   Executing funcs and storing imgurl list in new file\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "for x in mainlist:\n",
    "    geturlThreadPage(x)\n",
    "\n",
    "\n",
    "for x in sublist:\n",
    "    geturlImgPage(x,imglist)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#with open(\"imglist.txt\", \"w\",encoding=\"utf-8\") as output: \n",
    "    #output.write(str(imglist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Example:\n",
    "    \n",
    "['archiver/?pid-42.html', 'archiver/?pid-05.html', 'archiver/?pid-96.html', 'archiver/?pid-86.html', 'archiver/?pid-73.html', 'archiver/?pid-69.html', 'archiver/?pid-50.html', 'archiver/?pid-46.html', 'archiver/?pid-31.html', 'archiver/?pid-25.html', 'archiver/?pid-15.html', 'archiver/?pid-91.html', 'archiver/?pid-81.html', 'archiver/?pid-57.html', 'archiver/?pid-65.html', 'archiver/?pid-54.html', 'archiver/?pid-42.html', 'archiver/?pid-33.html', 'archiver/?pid-27.html', 'archiver/?pid-19.html']\n",
    "\n",
    "Breaking loop athttp://www.somekindofwebsite.jp//archiver/?pid-42-page-3.html\n",
    "\n",
    "No Imgs Found on 1st Page. Going next thread.\n",
    "\n",
    "Adding: https://www.someurlhost.com/2020/08/04/fbeuifu.jpg\n",
    "\n",
    "Adding: https://www.someurlhost.com/2020/08/04/dwqfewe.jpg\n",
    "\n",
    "Adding: https://www.someurlhost.com/2020/08/04/gerrgege8.jpg\n",
    "\n",
    "Adding: https://www.someurlhost.com/2020/08/04/ewqeqweqw.jpg\n",
    "\n",
    "Adding: https://www.someurlhost.com/2020/08/04/fewgregeg.jpg\n",
    "\n",
    "No Page Found. Breaking loop at: http://www.somekindofwebsite.jp//archiver/?pid-34553-page-2.html\n",
    "\n",
    "No Imgs Found on 1st Page. Going next thread.\n",
    "\n",
    "No Imgs Found on 1st Page. Going next thread.\n",
    "\n",
    "No Imgs Found on 1st Page. Going next thread.\n",
    "\n",
    "No Imgs Found on 1st Page. Going next thread.\n",
    "\n",
    "No Imgs Found on 1st Page. Going next thread.\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script is not fully automated yet. Still I need to sort and clean urls manually.\n",
    "I have now a file with all urls (some dead urls, some hosts might be very slow). First I would like to clean. \n",
    "Below cell is proposition how to organize urls - It's just an option. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "photos = open(\"zdjecia2.txt\", \"r\",encoding = \"utf-8\")\n",
    "imglist = photos.read().split(\"\\n\")\n",
    "\n",
    "#print(f'Amount of urls: {len(imglist)}')\n",
    "imglist = set(imglist)         #Eliminate all repeative urls.\n",
    "imglist = list(imglist)\n",
    "#print(f'Amount of urls: {len(imglist)}')\n",
    "\n",
    "netloclist = []                #List to store all netloc urls. \n",
    "for url in imglist:\n",
    "    link = urlparse(url)\n",
    "    netloclist.append(link.netloc)\n",
    "\n",
    "cnt = Counter()\n",
    "for url in netloclist:\n",
    "    cnt[url] += 1\n",
    "dict(sorted(cnt.items(), key=lambda item: item[1], reverse = True)) #Found on Stack overflow\n",
    "                                                                    #https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output example:\n",
    "\n",
    "Amount of urls: 515550\n",
    "\n",
    "Amount of urls: 453813\n",
    "\n",
    "{'imgur.net': 115851,\n",
    "\n",
    " 'castles.com': 61228,\n",
    " \n",
    " 'pics.jp': 17881,\n",
    " \n",
    " 'img070.com': 13310,\n",
    " \n",
    " 'jpnet.com': 11872,\n",
    " \n",
    " ...} etc.\n",
    " \n",
    "\n",
    "I am planning to add to the script list of urls with long connection, errors and timeouts. Every 500-1000(user input) checked \n",
    "urls and depending on a decision from the user -> use function to delete all urls with not convenient urls/continue or never ask again.\n",
    "\n",
    "For now netlocks with big numbers I check manually 10 examples. If everything is fine I go to the next one until small numbers occur like let's say 5k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import urllib\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "#                                Some lists/variables\n",
    "#//////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "counturl = 0                  #Counters to get updated on status.\n",
    "imgstored = 0\n",
    "sizetotal = 0\n",
    "sizestored = 0\n",
    "# avoidlinks = [\"somewebsite.jpg\",'iamslow.png']   # Beginner way of manually avoiding urls. Not needed anymore - For future learning not deleted yet.\n",
    "directory = \"E:\\ScrapedImgs\\_\"\n",
    "chckforurl = \"\" #Variable to store chcked url. If host was slow - not waste time - set timeout to quick one. \n",
    "                #Else 5 secs - Some images might be big and take time to download.\n",
    "\n",
    "def download_img(url,directory):    \n",
    "#Function taking url - connecting to website and then storing image into specified directory.\n",
    "#I will be working to make it into class.\n",
    "    \n",
    "    # global avoidlinks\n",
    "    global counturl\n",
    "    global imgstored\n",
    "    global sizetotal\n",
    "    global sizestored\n",
    "    global chckforurl\n",
    "    a = urlparse(url)\n",
    "    b = \"\".join(a.path.split(\"/\")) #For the name of picture file.\n",
    "    counturl += 1\n",
    "    if a.netloc == chckforurl:\n",
    "        timeval = 0.2\n",
    "    else:\n",
    "        timeval = 5\n",
    "    try:\n",
    "        response = requests.get(url,allow_redirects = False, stream = True, timeout = timeval)     \n",
    "    except HTTPError: \n",
    "        return(\"HTTPError\")\n",
    "    except URLError:\n",
    "        return('The server could not be found!')\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(url)\n",
    "        chckforurl = a.netloc\n",
    "        return(\"Timeout occurred\")            \n",
    "    except ConnectionError:  \n",
    "        return(\"ConnectionError\")      \n",
    "    except:\n",
    "        print(f\"Could not retrieve image! Image {a.path} not added.\")\n",
    "        return None\n",
    "    if \"[/img]\" in a.path:              #Bad formating from users in BB code - Couple of urls might be messed up.\n",
    "        return None\n",
    "         \n",
    "    \n",
    "    if '?' in b or \"*\" in b or \":\" in b:  #Not accepted symbols for creating file names.\n",
    "        b = b[:b.find('?')]\n",
    "        b = b[:b.find(\"*\")]\n",
    "        b = b[:b.find(\":\")]\n",
    "        \n",
    "        \n",
    "    if \".\" in a.path[-5:-3]:\n",
    "        symbol = True\n",
    "        file = open(f\"{directory}{b}\", \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "        size =((os.stat(f\"{directory}{b}\").st_size)/1024)\n",
    "        sizetotal += (size/1024)\n",
    "    \n",
    "    \n",
    "    else: \n",
    "        symbol = False\n",
    "        file = open(f\"{directory}{b}.jpg\", \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "        size =((os.stat(f\"{directory}{b}.jpg\").st_size)/1024)\n",
    "        sizetotal += (size/1024)\n",
    "       \n",
    "    if size < 60:\n",
    "        if symbol:\n",
    "            os.remove(f\"{directory}{b}\")\n",
    "    \n",
    "               \n",
    "        else:\n",
    "            try:\n",
    "                os.remove(f\"{directory}{b}.jpg\")\n",
    "            except:\n",
    "                print(\"Sth went wrong!\")\n",
    "           \n",
    "              \n",
    "    else:\n",
    "        sizestored += (size/1024)\n",
    "        imgstored += 1\n",
    "        print(\"Creating file: ...{} // Size: {} KB // Imgs_Stored: {} // Urls_chcked {} //Size stored: {} MB /Total: {} MB\".format(b[-8:],round(size,2),imgstored,counturl,round(sizestored,2),round(sizetotal,2)))\n",
    " \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in imglist:\n",
    "    try:\n",
    "        download_img(x,\"E:\\ScrapedImgs\\_\")\n",
    "    except:\n",
    "        print(\"Sth is wrong\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output example:\n",
    "\n",
    "Could not retrieve image! Image /api/cmfs/3/543/ogp/120.jpg not added.\n",
    "\n",
    "Could not retrieve image! Image  not added.\n",
    "\n",
    "https://www.somekind/blehiwe4.jpg\n",
    "\n",
    "Timeout occurred\n",
    "\n",
    "Creating file: ...jp-1.jpg // Size: 70.23 KB // Imgs_Stored: 55100 // Urls_chcked 244372 //Size stored: 10093.35 MB /Total: 11211.59 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
